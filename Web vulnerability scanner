# vulnerability_scanner.py

import os
import hashlib
import json
import argparse
import sys
import time
from urllib.parse import urljoin, urlparse
from bs4 import BeautifulSoup
import requests
import re
from typing import Dict, List, Any, Optional

# Constants
DATA_STORE_FILE = "scan_results.json"
FIM_DB_FILE = "fim_hashes.json"
CHUNK_SIZE = 8192 # Optimal for I/O operations [1, 2]
SQLI_PAYLOADS =
XSS_PAYLOADS =

# Install required external libraries
# pip install requests beautifulsoup4 blake3

try:
    import blake3
except ImportError:
    print("The 'blake3' library is not installed. Please install it using 'pip install blake3' for enhanced performance.")
    print("Falling back to SHA-256 for File Integrity Monitoring.")
    blake3 = None

class DataStore:
    """
    A file-based key-value store using JSON for persistence.
    This class encapsulates all data handling logic for the scanner.[3, 4]
    """
    def __init__(self, filename: str):
        self.filename = filename
        self.data = self._load_data()

    def _load_data(self) -> Dict[str, Any]:
        """Loads data from the JSON file. Handles missing or corrupted files.[3, 5, 4]"""
        if not os.path.exists(self.filename):
            return {}
        try:
            with open(self.filename, 'r', encoding='utf-8') as f:
                return json.load(f)
        except (json.JSONDecodeError, FileNotFoundError, IOError) as e:
            print(f"Warning: Could not load data from '{self.filename}'. The file may be corrupted or empty. Creating a new one. Error: {e}")
            return {}

    def _save_data(self) -> None:
        """Saves the current data to the JSON file.[3, 4]"""
        try:
            with open(self.filename, 'w', encoding='utf-8') as f:
                json.dump(self.data, f, indent=4)
        except IOError as e:
            print(f"Error: Could not save data to '{self.filename}'. Error: {e}")
            sys.exit(1)

    def set(self, key: str, value: Any) -> None:
        """Sets or updates a key-value pair and saves the data.[3, 4]"""
        self.data[key] = value
        self._save_data()

    def get(self, key: str) -> Optional[Any]:
        """Retrieves a value by key, returning None if not found.[3, 4]"""
        return self.data.get(key)

class FileIntegrityMonitor:
    """
    A tool to monitor the integrity of the data store file using cryptographic hashing.
    It uses BLAKE3 for performance if available, otherwise falls back to SHA-256.[6, 7]
    """
    def __init__(self, target_file: str, db_file: str):
        self.target_file = target_file
        self.db_file = db_file
        self.store = DataStore(db_file)

    def _calculate_hash(self, file_path: str, algorithm: str = 'sha256') -> Optional[str]:
        """
        Computes the hash of a file in chunks to efficiently handle large files.[1, 2]
        Uses hashlib.new() for algorithm flexibility.[8, 9]
        """
        try:
            if blake3 and algorithm == 'blake3':
                hasher = blake3.blake3()
            else:
                hasher = hashlib.new(algorithm)
            
            with open(file_path, "rb") as f:
                while chunk := f.read(CHUNK_SIZE):
                    hasher.update(chunk)
            
            return hasher.hexdigest()
        except (FileNotFoundError, PermissionError) as e:
            print(f"Error: Could not access file '{file_path}'. Check permissions. {e}", file=sys.stderr)
            return None
        except Exception as e:
            print(f"An unexpected error occurred while hashing '{file_path}': {e}", file=sys.stderr)
            return None

    def init_baseline(self) -> None:
        """Establishes a new integrity baseline for the data store file."""
        print(f"Creating new integrity baseline for '{self.target_file}'...")
        file_hash = self._calculate_hash(self.target_file, 'blake3' if blake3 else 'sha256')
        if file_hash:
            self.store.set("integrity_hash", file_hash)
            print("Baseline successfully created.")
        else:
            print("Failed to create baseline.")

    def check_integrity(self) -> bool:
        """
        Compares the current hash of the file against the stored baseline.
        Returns True if hashes match, False otherwise.[10, 11]
        """
        print(f"Checking integrity of '{self.target_file}'...")
        stored_hash = self.store.get("integrity_hash")
        if not stored_hash:
            print("Error: No integrity baseline found. Please run the 'init' command first.")
            return False

        current_hash = self._calculate_hash(self.target_file, 'blake3' if blake3 else 'sha256')
        if not current_hash:
            print("Integrity check failed: Unable to compute current hash.")
            return False
            
        if current_hash == stored_hash:
            print("Integrity Check Passed: The data store file has not been altered.")
            return True
        else:
            print("Integrity Check FAILED: The data store file has been altered or corrupted.")
            return False

class WebCrawler:
    """
    A modular web crawler to discover URLs and input forms.
    It uses requests.Session for performance and cookie management.[12, 13, 14]
    """
    def __init__(self, session: requests.Session):
        self.session = session
        self.visited_urls = set()
        self.found_vectors = {}

    def _get_page(self, url: str) -> Optional[str]:
        """Fetches the HTML content of a URL with proper error handling."""
        try:
            response = self.session.get(url, timeout=10)
            response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx) [15]
            return response.text
        except requests.exceptions.RequestException as e:
            print(f"Error fetching URL '{url}': {e}", file=sys.stderr)
            return None

    def _find_vectors(self, html_content: str, base_url: str) -> List]:
        """Parses HTML to find all forms, links, and inputs.[16, 17]"""
        soup = BeautifulSoup(html_content, 'html.parser')
        vectors =
        
        # Find all forms and their inputs
        for form in soup.find_all('form'):
            form_data = {
                'type': 'form',
                'action': urljoin(base_url, form.get('action', '')),
                'method': form.get('method', 'GET').upper(),
                'fields': {}
            }
            # Find all input fields within the form [16, 17]
            for input_tag in form.find_all(['input', 'textarea', 'select']):
                input_name = input_tag.get('name')
                if input_name:
                    form_data['fields'][input_name] = input_tag.get('value', '')
            vectors.append(form_data)
        
        # Find all links (a tags) [18, 19, 20]
        for link in soup.find_all('a'):
            href = link.get('href')
            if href:
                full_url = urljoin(base_url, href)
                # Ensure the link is within the same domain
                if urlparse(full_url).netloc == urlparse(base_url).netloc:
                    vectors.append({
                        'type': 'link',
                        'url': full_url,
                        'method': 'GET'
                    })
        return vectors

    def crawl(self, start_url: str) -> Dict[str, Any]:
        """Starts the crawling process from a given URL."""
        if start_url in self.visited_urls:
            return
        
        print(f"Crawling: {start_url}")
        self.visited_urls.add(start_url)
        
        html_content = self._get_page(start_url)
        if not html_content:
            return
        
        found_vectors = self._find_vectors(html_content, start_url)
        self.found_vectors[start_url] = found_vectors
        
        # Recursively crawl found links
        for vector in found_vectors:
            if vector['type'] == 'link':
                self.crawl(vector['url'])
                
        return self.found_vectors
        
class VulnerabilityScanner:
    """
    Core engine for vulnerability checks, including XSS and SQL Injection.
    """
    def __init__(self, session: requests.Session):
        self.session = session
        self.vulnerabilities = {
            'xss':,
            'sqli':
        }
        self.db_store = DataStore(DATA_STORE_FILE)

    def _test_xss(self, url: str, method: str, data: Dict[str, str]) -> None:
        """
        Tests for Reflected XSS by injecting payloads and checking if they appear
        unmodified in the response body.[21, 22, 23]
        """
        for payload in XSS_PAYLOADS:
            test_data = data.copy()
            
            # Inject payload into each parameter
            for key in test_data:
                test_data[key] = payload
            
            if method == 'POST':
                response = self.session.post(url, data=test_data)
            else:
                response = self.session.get(url, params=test_data)
            
            # Check if the payload is present in the response body
            if payload in response.text:
                self.vulnerabilities['xss'].append({
                    'vulnerability': 'Reflected XSS',
                    'url': url,
                    'method': method,
                    'payload': payload
                })

    def _test_sqli(self, url: str, method: str, data: Dict[str, str]) -> None:
        """
        Tests for SQL Injection (Error-based and Time-based Blind).[24, 25, 26, 27, 28]
        """
        # Test for common error-based SQLi
        for payload in SQLI_PAYLOADS:
            test_data = data.copy()
            for key in test_data:
                test_data[key] = payload
            
            if method == 'POST':
                response = self.session.post(url, data=test_data)
            else:
                response = self.session.get(url, params=test_data)
            
            # Look for common database error signatures
            if any(err_msg in response.text for err_msg in):
                self.vulnerabilities['sqli'].append({
                    'vulnerability': 'Error-based SQLi',
                    'url': url,
                    'method': method,
                    'payload': payload,
                    'details': 'Database error message detected.'
                })
        
        # Test for Time-based Blind SQLi
        # Create a time-based payload for different databases [26, 27]
        time_payloads =
        
        for payload in time_payloads:
            test_data = data.copy()
            for key in test_data:
                test_data[key] = payload
            
            start_time = time.time()
            if method == 'POST':
                self.session.post(url, data=test_data)
            else:
                self.session.get(url, params=test_data)
            end_time = time.time()
            
            # If the response takes significantly longer, it's a vulnerability [26, 28]
            if (end_time - start_time) > 4: # Threshold of 4 seconds to account for network latency
                self.vulnerabilities['sqli'].append({
                    'vulnerability': 'Time-based Blind SQLi',
                    'url': url,
                    'method': method,
                    'payload': payload,
                    'details': 'Response time was significantly delayed.'
                })

    def run_scan(self, start_url: str):
        """Orchestrates the crawling and scanning process."""
        print(f"Starting scan on {start_url}...")
        session = requests.Session()
        crawler = WebCrawler(session)
        found_vectors = crawler.crawl(start_url)
        
        if not found_vectors:
            print("No input vectors found on the site. Scan complete.")
            return

        print("\nFound input vectors. Beginning vulnerability tests...")
        for url, vectors in found_vectors.items():
            for vector in vectors:
                if vector['type'] == 'form':
                    print(f"Scanning form at {url} (Method: {vector['method']})")
                    self._test_xss(vector['action'], vector['method'], vector['fields'])
                    self._test_sqli(vector['action'], vector['method'], vector['fields'])
        
        self.db_store.set("scan_results", self.vulnerabilities)
        
        print("\nScan complete. Results saved to 'scan_results.json'.")
        print("Summary of findings:")
        print(f"  - XSS Vulnerabilities Found: {len(self.vulnerabilities['xss'])}")
        print(f"  - SQLi Vulnerabilities Found: {len(self.vulnerabilities['sqli'])}")

def main():
    """Main function to handle command-line operations."""
    parser = argparse.ArgumentParser(
        description="A Web Application Vulnerability Scanner to detect common flaws like SQLi and XSS."
    )
    
    subparsers = parser.add_subparsers(dest="command", required=True, help="Available commands")
    
    # Subparser for the 'scan' command
    scan_parser = subparsers.add_parser("scan", help="Run a vulnerability scan on a target URL.")
    scan_parser.add_argument("url", help="The starting URL for the scan.")
    
    # Subparser for the 'check-integrity' command
    check_integrity_parser = subparsers.add_parser("check-integrity", help="Verify the integrity of the scan results file.")

    # Subparser for the 'init-integrity' command
    init_integrity_parser = subparsers.add_parser("init-integrity", help="Establish the integrity baseline for the scan results file.")
    
    args = parser.parse_args()
    
    if args.command == "scan":
        scanner = VulnerabilityScanner(requests.Session())
        scanner.run_scan(args.url)
    elif args.command == "check-integrity":
        fim = FileIntegrityMonitor(DATA_STORE_FILE, FIM_DB_FILE)
        fim.check_integrity()
    elif args.command == "init-integrity":
        fim = FileIntegrityMonitor(DATA_STORE_FILE, FIM_DB_FILE)
        fim.init_baseline()

if __name__ == "__main__":
    main()
